{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbffed5",
   "metadata": {},
   "source": [
    "# SOTA model for object detection - RetinaNet\n",
    "\n",
    "![RetinaNet results](https://www.researchgate.net/profile/Xuxi-Yang/publication/342017040/figure/fig4/AS:901688093310976@1591990602471/sualized-detection-results-of-RetinaNet-trained-model.ppm)\n",
    "<center>Image taken from <a href=\"https://www.researchgate.net/figure/sualized-detection-results-of-RetinaNet-trained-model_fig4_342017040\">here</a></center>\n",
    "<br><br>\n",
    "\n",
    "This project aims to walk through the main parts of the **RetinaNet** - one of the best architectures for the object detection task. \n",
    "\n",
    "The full implementation of this model can be very long, and it can take even several weeks to cover everything that goes into it. The RetinaNet is an upgrade on the previous iterations such as SSD (Singe-Shot Detector), so we will cover and implement the main parts that made RetinaNet better than the rest of the models in this project category. \n",
    "\n",
    "#### We recommend going over the original implementation in Keras on your own time if you are interested in applying this model to your project. \n",
    "\n",
    "\n",
    "### Example of RetinaNet for self-driving car\n",
    "\n",
    "RetinaNet works extremely fast, and because of that, many self-driving car projects are using it to describe its surroundings. Here is a video about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff7267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19147c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/KYueHEMGRos\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f28ca85cc8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/KYueHEMGRos\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f5375",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case, here is the like for YouTube video from the cell above: https://www.youtube.com/embed/KYueHEMGRos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274bd937",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "![RetinaNet](https://www.researchgate.net/publication/327737749/figure/fig1/AS:672393336987655@1537322472864/The-network-architecture-of-RetinaNet-RetinaNet-uses-the-Feature-Pyramid-Network-FPN.png)\n",
    "\n",
    "<center>Image taken from <a href=\"https://www.researchgate.net/figure/The-network-architecture-of-RetinaNet-RetinaNet-uses-the-Feature-Pyramid-Network-FPN_fig1_327737749\">here</a></center>\n",
    "<br><br>\n",
    "\n",
    "### Paper overview of RetinaNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "026f606e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/infFuZ0BwFQ\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f28ca85b08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/infFuZ0BwFQ\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048bc7c",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case, here is the like for YouTube video from the cell above: https://www.youtube.com/embed/infFuZ0BwFQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335cd4a",
   "metadata": {},
   "source": [
    "\n",
    "RetinaNet has 4 main parts, and we will cover all of them in this project.\n",
    "\n",
    "\n",
    "### Steps:\n",
    "1. ResNet Backbone\n",
    "2. Feature Pyramid Network (FPN)\n",
    "3. Classification and Regression heads\n",
    "4. Focal loss\n",
    "\n",
    "### Topics covered and learning objectives\n",
    "- ResNet\n",
    "- Pyramid Networks\n",
    "- Multi-head networks\n",
    "- RetinaNet\n",
    "- Focal Loss\n",
    "\n",
    "### Time estimates:\n",
    "- Reading/Watching materials: 2h 20min\n",
    "- Exercises: 1h\n",
    "<br><br>\n",
    "- **Total**: ~3.5h\n",
    "\n",
    "\n",
    "\n",
    "RetinaNet intuition: https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d\n",
    "\n",
    "### Imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c97971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, Layer, UpSampling2D, ReLU, Input\n",
    "\n",
    "from tests import TEST_BACKBONE_MODEL, TEST_RESNET, TEST_HEAD_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ebf7e3",
   "metadata": {},
   "source": [
    "### Exercise 1: Create ResNet50 backbone\n",
    "\n",
    "![](images/resnet.png)\n",
    "\n",
    "The original version of RetinaNet uses a pre-trained backbone - ResNet50. This network is used to make features for the Feature Pyramid Network (FPN). \n",
    "\n",
    "The goal of FPN is to generate anchor proposals of different sizes and scales. Having only the final output from the backbone (in this case, it's ResNet50, but you can use almost any feature generator as long as it matches with the FPN) won't provide enough context about the image itself. To overcome this challenge, we will output features (layer outputs) from several parts of the ResNet50 model and use them within the FPN to better image representations.\n",
    "\n",
    "Your task is to implement the **get_backbone** function that returns a custom Model with multiple outputs. \n",
    "\n",
    "Here are the points to follow:\n",
    "\n",
    "1. Define ResNet50 object with input_shape of 128x128x3 and do not include the *top*\n",
    "2. We need to extract specific layers from the model. You can do that with model.get_layer(\"name_of_the_layer\"). Since we want to return these as the output, do not forget to add **.output** at the end. \n",
    "    \n",
    "    Following the paper, here are layer names to extract:\n",
    "\n",
    "    - conv3_block4_out\n",
    "    - conv4_block6_out\n",
    "    - conv5_block3_out\n",
    "    \n",
    "    \n",
    "3. Return keras Model with inputs as inputs of the backbone network, and outputs are three extracted layers from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47c03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    \"\"\"\n",
    "    Create ResNet50 backbone with several outputs used in the Feature Pyramid Network\n",
    "    \"\"\"\n",
    "    backbone = ResNet50(include_top=False, input_shape=[128, 128, 3])\n",
    "    \n",
    "    c3_output = backbone.get_layer(\"conv3_block4_out\").output\n",
    "    c4_output = backbone.get_layer(\"conv4_block6_out\").output\n",
    "    c5_output = backbone.get_layer(\"conv5_block3_out\").output\n",
    "    \n",
    "    return Model(inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df69de72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "TEST_BACKBONE_MODEL(get_backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786470d",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Pyramid Network \n",
    "\n",
    "![](https://miro.medium.com/max/500/1*XmNDHT8WWZbXACyBjg3ZeQ.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59a8de69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/mwMopcSRx1U\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f28ca7b148>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/mwMopcSRx1U\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc108d",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case, here is the like for YouTube video from the cell above: https://www.youtube.com/watch?v=mwMopcSRx1U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5131177b",
   "metadata": {},
   "source": [
    "Further reading about FPNs:\n",
    "- First read this: https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610\n",
    "- Read this for a detailed intro: https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*edviRcl3vwlyx9TS_gRbmg.png)\n",
    "\n",
    "\n",
    "In the cell below, you'll find FPN already started with layers defined. Your task is to connect them inside the **call** method. The goal of this exercise is to learn how connected FPNs layers are.\n",
    "\n",
    "Here is how:\n",
    "- 1st, Put all 3 outputs from the backbone through 1x1 convolutions\n",
    "- 2nd merge p4_1x1 with upsampled p5_1x1\n",
    "- 3rd merge p3_1x1 with upsampled merged_p4\n",
    "- 4th create p3-p7 outputs by using corresponding 3x3 Conv layers and inputs to them \n",
    "    - for p3 and p4, we use merged versions\n",
    "    - for p7, use p6_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1baf23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePyramidNetwork(Layer):\n",
    "    \"\"\"\n",
    "    Builds the Feature Pyramid with the feature maps from the backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FeaturePyramidNetwork, self).__init__(name=\"FeaturePyramidNetwork\", **kwargs)\n",
    "        \n",
    "        self.backbone = get_backbone()\n",
    "        \n",
    "        # 1x1 convolution layers\n",
    "        self.conv_c3_1x1 = Conv2D(filters=256, kernel_size=1, strides=1, padding=\"same\")\n",
    "        self.conv_c4_1x1 = Conv2D(filters=256, kernel_size=1, strides=1, padding=\"same\")\n",
    "        self.conv_c5_1x1 = Conv2D(filters=256, kernel_size=1, strides=1, padding=\"same\")\n",
    "        \n",
    "        # 3x3 convo layers\n",
    "        self.conv_c3_3x3 = Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\")\n",
    "        self.conv_c4_3x3 = Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\")\n",
    "        self.conv_c5_3x3 = Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\")\n",
    "        self.conv_c6_3x3 = Conv2D(filters=256, kernel_size=3, strides=2, padding=\"same\")\n",
    "        self.conv_c7_3x3 = Conv2D(filters=256, kernel_size=3, strides=2, padding=\"same\", activation='relu')\n",
    "        \n",
    "        self.upsample_2x = UpSampling2D(2)\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        \n",
    "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
    "         \n",
    "        # Reduce image depth with 1x1 convolutions\n",
    "        p3_1x1 = self.conv_c3_1x1(c3_output)\n",
    "        p4_1x1 = self.conv_c4_1x1(c4_output)\n",
    "        p5_1x1 = self.conv_c5_1x1(c5_output)\n",
    "            \n",
    "        # Merge p4 1x1 reduced with upsampled p5 1x1 \n",
    "        merged_p4 = p4_1x1 + self.upsample_2x(p5_1x1)\n",
    "        \n",
    "        # Merge p3 1x1 with upsampled merged P4\n",
    "        merged_p3 = p3_1x1 + self.upsample_2x(merged_p4)\n",
    "        \n",
    "        # Make p3-p6 outputs with 3x3 convs\n",
    "        p3_output = self.conv_c3_3x3(merged_p3)\n",
    "        p4_output = self.conv_c4_3x3(merged_p4)\n",
    "        p5_output = self.conv_c5_3x3(p5_1x1)\n",
    "        p6_output = self.conv_c6_3x3(p5_output)\n",
    "        p7_output = self.conv_c7_3x3(p6_output)\n",
    "        \n",
    "        return p3_output, p4_output, p5_output, p6_output, p7_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76addf",
   "metadata": {},
   "source": [
    "### Exercise 3: Network heads\n",
    "\n",
    "![](images/heads.png)\n",
    "\n",
    "The last part of the RetinaNet architecture is prediction heads - one for classification and the other for regression (bbox prediction).\n",
    "\n",
    "![](https://miro.medium.com/max/2400/1*rYvoP6VcmMGVQdIsaSawdQ.png)\n",
    "\n",
    "Each of these heads receives the same input, but its task is a bit different. The classification network can predict the correct class for a specific place of the image, whereas the regression network predicts the coordinates of anchors. \n",
    "\n",
    "To understand how to calculate the number of outputs of these heads, go back to this link: https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d\n",
    "\n",
    "Your task in this exercise is to build a generic head of the RetinaNet. \n",
    "\n",
    "The architecture is straightforward. Start by adding the Input layer to the **head** (Sequential model defined for you). Input shape should be [None, None, 256].\n",
    "\n",
    "When you are done with that, add **four (4)** convolutional layers with the same set of parameters:\n",
    "- filters=256\n",
    "- kernel_size=3\n",
    "- padding=\"same\"\n",
    "- activation=\"relu\"\n",
    "\n",
    "To complete the head network, add one more convolutional layer, which will be the output of this model. For this layer set *filters* to be *output_filters*, kernel_size=3, stride=1 and padding='same'. We don't want any activation function here.\n",
    "\n",
    "Return the model head from this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67b7be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_head(output_filters):\n",
    "    \"\"\"Builds the class/box predictions head.\n",
    "\n",
    "    Arguments:\n",
    "      output_filters: Number of convolution filters in the final layer.\n",
    "\n",
    "    Returns:\n",
    "      A keras sequential model representing either the classification \n",
    "      or the box regression head depending on `output_filters`.\n",
    "    \"\"\"\n",
    "    head = Sequential([Input(shape=[None, None, 256])])\n",
    "    \n",
    "    head.add(Conv2D(256, 3, padding=\"same\", activation=\"relu\"))\n",
    "    head.add(Conv2D(256, 3, padding=\"same\", activation=\"relu\"))\n",
    "    head.add(Conv2D(256, 3, padding=\"same\", activation=\"relu\"))\n",
    "    head.add(Conv2D(256, 3, padding=\"same\", activation=\"relu\"))\n",
    "    \n",
    "    head.add(Conv2D(output_filters, 3, 1, padding=\"same\" ))\n",
    "    \n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eedc9488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "TEST_HEAD_MODEL(build_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a56af3",
   "metadata": {},
   "source": [
    "### Exercise 4: Building the RetinaNet model\n",
    "\n",
    "You have created all the building blocks for the RetinaNet model and gone through materials about it. It's time to put everything together!\n",
    "\n",
    "*This exercise has 2 parts*\n",
    "\n",
    "### Part 1: __ init__ method\n",
    "\n",
    "You'll find the starting point for the **RetinaNet** class with two methods (*__ init __* and *call*) in the cell below.\n",
    "\n",
    "In this part of the exercise, your task is to complete the *init* method following the next steps:\n",
    "\n",
    "- Define a variable that holds a number of classes (provided as an argument of the init method)\n",
    "- Create an object of the *FeaturePyramidNetwork*\n",
    "- Create classification head object using function **build_head** with argument: num_anchors * num_classes\n",
    "- Create regression head object using function **build_head** with argument: num_anchors * 4\n",
    "\n",
    "### Part 2: Full model in the **call** method\n",
    "\n",
    "For part two, your task is to complete the **call** function. Here is what you need to do:\n",
    "NOTE: Two empty lists to store outputs from the class head and regression head are created for you\n",
    "\n",
    "1. Generate features using the FPN model defined in the __init__ method\n",
    "2. Since FPN will generate features for many sizes, create a for loop that iterates over them\n",
    "3. Inside the for-loop, call the regression head from the *init* method on the feature, reshape the output to be **batch_size, -1, 4** (this size is used for the loss function)\n",
    "5. Append the output from step 4 to the list dedicated for regression head outputs\n",
    "6. Do the same process for the classification head, only this time reshape the output to be **batch_size, -1, self.num_classes**\n",
    "7. Using the TensorFlow method, concatenate all tensors from **cls_outputs** along axis=1, and assign that to cls_outputs at the end of the call method\n",
    "8. Using the TensorFlow method, concatenate all tensors from **box_outputs** along axis=1, and assign that to box_outputs at the end of the call method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e876714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNet(Model):\n",
    "    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n",
    "\n",
    "    Attributes:\n",
    "      num_classes: Number of classes in the dataset.\n",
    "      backbone: The backbone to build the feature pyramid from.\n",
    "        Currently supports ResNet50 only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_anchors=9, **kwargs):\n",
    "        super(RetinaNet, self).__init__(name=\"RetinaNet\", **kwargs)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.fpn = FeaturePyramidNetwork()\n",
    "        \n",
    "        # 9 is the number of different rations and sizes for anchor boxes\n",
    "        self.cls_head = build_head(num_anchors * num_classes)\n",
    "        self.box_head = build_head(num_anchors * 4)\n",
    "\n",
    "    def call(self, image, training=False):\n",
    "        batch_size = tf.shape(image)[0]\n",
    "        # Empty lists used to store head outputs\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        features = self.fpn(image, training=training)\n",
    "    \n",
    "        for feature in features:\n",
    "            box_outputs.append(tf.reshape(self.box_head(feature), [batch_size, -1, 4]))\n",
    "            cls_outputs.append(tf.reshape(self.cls_head(feature), [batch_size, -1, self.num_classes]))\n",
    "            \n",
    "            \n",
    "        # DON'T CHANGE CODE BELOW THIS LINE\n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        \n",
    "        return tf.concat([box_outputs, cls_outputs], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e92b62",
   "metadata": {},
   "source": [
    "### Execute these two lines to see if everything compiles for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d694728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  RetinaNet(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0c2d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer RetinaNet is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "TEST_RESNET(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c9258",
   "metadata": {},
   "source": [
    "## Focal loss\n",
    "\n",
    "The last step, before we compile our model, we will need to mention and learn about Focal loss, the last part that holds everything together.\n",
    "\n",
    "![](https://lh4.googleusercontent.com/_Zb8VyevBHbPdlPS1Bcph18b0GnRdY__yrSWaxEobHAOSq5izCVXdRS0Eo-26pU5Q8JE2daQAmFlwwUKnRiaf7JJrv7VJOLXbTOF-B6G8yshVWdBwhRXFBuMB5L6eH7KCTjzen-t7e39pxku5A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76267a34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/Y8_OVwK4ECk\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f28efdaf48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/Y8_OVwK4ECk\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b562206",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case, here is the like for YouTube video from the cell above: https://www.youtube.com/embed/Y8_OVwK4ECk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16e527",
   "metadata": {},
   "source": [
    "Focal-loss tutorial: https://www.analyticsvidhya.com/blog/2020/08/a-beginners-guide-to-focal-loss-in-object-detection/\n",
    "\n",
    "Focal-loss implementations: https://www.programmersought.com/article/60001511310/\n",
    "\n",
    "The implementation of Focal-loss is a bit complex to put here, and we would need many more util functions to enable it. Luckily, a Python library called **focal-loss** allows us to import Keras compliant focal-loss and put it to our model.\n",
    "\n",
    "If you don't have it installed, uncomment the code in the cell below and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install focal-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4671837a",
   "metadata": {},
   "source": [
    "Using the *focal-loss* library we are able to import it for Binary classification or Categorical classification in only one line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e391c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss import SparseCategoricalFocalLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e874977",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=SparseCategoricalFocalLoss(gamma=2), optimizer=\"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b73328",
   "metadata": {},
   "source": [
    "## Where from here?\n",
    "\n",
    "- Full keras implementation of RetinaNet: https://keras.io/examples/vision/retinanet/\n",
    "- Keras RetinaNet library: https://github.com/cvisionai/keras_retinanet\n",
    "- Pedestrian detection with RetinaNet: https://towardsdatascience.com/pedestrian-detection-in-aerial-images-using-retinanet-9053e8a72c6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

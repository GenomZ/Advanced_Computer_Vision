{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8441cd",
   "metadata": {},
   "source": [
    "# Face detection using Single Shot Detector (SSD)\n",
    "\n",
    "So far, you have worked with multiple models for object detection (R-CNN, Fast R-CNN, and Faster R-CNN), but we haven't discussed two families (approaches) of algorithms for object detection.\n",
    "\n",
    "There are algorithms considered to be **two-stage detectors** (e.g. Faster R-CNN) and **single-stage detectors** (e.g. SSD, RetinaNet). The main difference is in the approach of finding the region of interest and finally making predictions.\n",
    "\n",
    "#### Two-stage detectors\n",
    "In two-stage detectors, models first predict region proposals and use them to make predictions. This process can be done as a joint part of the main architecture, one example of this model is Faster R-CNN, or it can be done separately - which slows the prediction time.\n",
    "\n",
    "#### Single-stage detectors\n",
    "With good optimization, two-stage detectors can reach a pretty decent prediction speed, almost in real-time. Still, this performance needed to be much more stable for some applications (e.g., Factory security cameras). That's where SSDs come into action. \n",
    "\n",
    "Algorithms from this family don't calculate ROIs, cropping them and making predictions based on those regions. Instead, these algorithms directly predict coordinates from the image. By removing the ROI calculation step, these algorithms are much faster in the prediction time and can be used for real-time detection. \n",
    "\n",
    "To learn more about the differences between these algorithms:\n",
    "\n",
    "- Algorithm comparison: https://github.com/yehengchen/Object-Detection-and-Tracking/blob/master/Two-stage%20vs%20One-stage%20Detectors.md\n",
    "- Stackoverflow answers: https://stackoverflow.com/questions/65942471/one-stage-vs-two-stage-object-detection\n",
    "- Single-shot Multibox predictors: https://medium.com/@ManishChablani/ssd-single-shot-multibox-detector-explained-38533c27f75f\n",
    "- SSD multibox object detection: https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105669f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30cf456b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/44tlnmmt3h0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1b3c72d51c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/44tlnmmt3h0\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9da37d",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case here is the like for YouTube video from the cell above: https://www.youtube.com/watch?v=44tlnmmt3h0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e22fe",
   "metadata": {},
   "source": [
    "## Task introduction\n",
    "\n",
    "![SSD](https://cdn-images-1.medium.com/max/1000/1*GmJiirxTSuSVrh-r7gtJdA.png)\n",
    "<center>Images taken from: <a href=\"https://developers.arcgis.com/python/guide/how-ssd-works/\">here</a></center>\n",
    "<br><br>\n",
    "\n",
    "In this notebook, you will implement a face detection application using a simplified version of the SSD algorithm, optimized to work well on detecting faces.\n",
    "\n",
    "![faces](https://i.stack.imgur.com/1Jyhp.png)\n",
    "<center>Images taken from: <a href=\"https://stackoverflow.com/questions/32586203/opencv-restrict-face-detection-only-in-correct-orientation\">here</a></center>\n",
    "<br><br>\n",
    "\n",
    "SSD Algorithm is the first from the single-shot (one-shot) object detection algorithms that you'll cover. It does not produce any ROIs but directly predicts coordinates of bounding boxes.\n",
    "\n",
    "\n",
    "### Steps:\n",
    "1. Import dependencies\n",
    "2. Downloading the dataset\n",
    "3. Processing images\n",
    "4. Training and test generators\n",
    "5. Convolutional block\n",
    "6. Residual Block\n",
    "7. SSD Model\n",
    "8. Training and prediction time\n",
    "\n",
    "### Topics covered and learning objectives\n",
    "- Sing-shot predictors/detectors\n",
    "- SSD model\n",
    "- Confidence and location losses\n",
    "- Residual blocks\n",
    "\n",
    "### Time estimates:\n",
    "- Reading/Watching materials: 3h 20min\n",
    "- Exercises: 1h 30min\n",
    "<br><br>\n",
    "- **Total**: ~5h\n",
    "\n",
    "\n",
    "### Imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d762fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePath, Path\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Input \n",
    "from tensorflow.keras.layers import Flatten, Dropout, BatchNormalization, Concatenate, Reshape, GlobalAveragePooling2D, Reshape\n",
    "\n",
    "# Custom helper functions\n",
    "# GO CHECK THEM OUT IN utils.py\n",
    "from utils import IoU, get_outputs, get_anchor, plot_example, plot_pred, get_outputs\n",
    "from tests import test_process_img, test_conv_block, test_residual_block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da79f87",
   "metadata": {},
   "source": [
    "## Let's download the dataset\n",
    "\n",
    "Dataset used in this tutorial is called Face Detection DataBase (FDDB), you can download it from this website - http://vis-www.cs.umass.edu/fddb/\n",
    "\n",
    "When you get there, you'll see a section that looks like this:\n",
    "![](images/download.png)\n",
    "\n",
    "Download both files - **Original, annotated set of images** and **Face annotations**. If you are done with this step, you should have two **zip** files:\n",
    "- originalPics.zip\n",
    "- FDDB-folds.zip\n",
    "\n",
    "Extract both of them in the working directory or **data/module_4** folder inside the root of the repo for the following to work!\n",
    "\n",
    "FDDB folder should have a lot of **txt** files, and the image directory two subfolders called **2002** and **2003**.\n",
    "\n",
    "This dataset has about **25.000** images, but we have one problem with their labels. Each face label is not a rectangle, but an ellipse. So before training the model, we have to convert those ellipses into bounding boxes.\n",
    "\n",
    "\n",
    "#### NOTE: For this task, we will use one size Anchor boxes of 4x4 because of the face shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_SIZE = 4\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d846ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all txt files with elipse annotations\n",
    "REPO_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "faces_dir = REPO_DIR / \"data/module_4/face_detection\"\n",
    "annotations_dir = REPO_DIR / \"data/module_4/FDDB-folds\"\n",
    "annot_files = glob.glob((annotations_dir / '*ellipseList.txt').__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b865fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was taken from the original repo for this dataset\n",
    "data = {}\n",
    "for file in annot_files:\n",
    "    with open(file, 'r') as f:\n",
    "        rows = f.readlines()\n",
    "        \n",
    "    j = len(rows)\n",
    "    i = 0   \n",
    "    while(i < j):\n",
    "        file_name = (faces_dir / (rows[i].replace('\\n', '')+'.jpg')).__str__()\n",
    "\n",
    "        # Number of bounding boxes\n",
    "        num_boxes = int(rows[i+1])\n",
    "        boxes = []\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(file_name), cv2.COLOR_BGR2RGB)\n",
    "        h, w, c = img.shape\n",
    "        \n",
    "        # Make sure that we take all ellipses per image \n",
    "        for k in range(1, num_boxes+1):\n",
    "            box = rows[i+1+k]\n",
    "            box = box.split()[0:5]\n",
    "            box = [float(x) for x in box]\n",
    "\n",
    "            # Convert ellipse to bounding box\n",
    "            xmin = int(box[3]- box[1])\n",
    "            ymin = int(box[4]- box[0])\n",
    "            xmax = int(xmin + box[1]*2)\n",
    "            ymax = int(ymin + box[0]*2)\n",
    "            boxes.append([xmin/w, ymin/h, xmax/w, ymax/h])\n",
    "        \n",
    "        # convert the boxes to a volume of fixed size \n",
    "        data[file_name] = get_outputs(boxes, ANCHOR_SIZE)\n",
    "        i = i + num_boxes+2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0114dd",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement image processing function\n",
    "\n",
    "In the cell below, complete the **process_img** function by following the following steps:\n",
    "- It should accept two arguments: path to the image and image_size, which is **integer**\n",
    "- Load it using OpenCV\n",
    "- Convert it from BGR to RGB\n",
    "- Resize the image to the image size provided as the argument (interpolation should be linear)\n",
    "- Normalize the picture, so all pixels are between 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bde2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img_path, img_size):\n",
    "    \"\"\"\n",
    "    Processes the input image.\n",
    "    \n",
    "    Processing steps:\n",
    "     - Load \n",
    "     - BGR -> RGB\n",
    "     - Resize\n",
    "     - Normalize\n",
    "     \n",
    "    Args:\n",
    "        img_path :string: - disk location of an image\n",
    "        img_size :int: - size of an image (e.g, 128)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "test_process_img(process_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35081c0",
   "metadata": {},
   "source": [
    "### Creating Training and Testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(data.keys())\n",
    "labels = list(data.values())\n",
    "X_train, X_test, y_train, y_test = train_test_split(files, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a911f6",
   "metadata": {},
   "source": [
    "### Training and testing generators\n",
    "\n",
    "These functions, called generators, are used to produce batches of data for the training or the testing part. For example, take a look at the **training_generator** and write another one for the testing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4928d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator():\n",
    "    \"\"\"\n",
    "    Creates a training set streamer used to batch dataset\n",
    "    \n",
    "    NOTE: USE YIELD INSTEAD OF RETURN\n",
    "    \"\"\"\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(len(X_train)):\n",
    "        img_path = X_train[i]\n",
    "        img = process_img(img_path, IMG_SIZE)\n",
    "        batch_images.append(img)\n",
    "        batch_labels.append(y_train[i])\n",
    "        counter += 1\n",
    "\n",
    "        if counter == BATCH_SIZE:\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "            counter = 0 \n",
    "            batch_images = []\n",
    "            batch_labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff23445",
   "metadata": {},
   "source": [
    "### Exercise 2: Take a look at the **training_generator** and write yours for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ae197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator():\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e3893",
   "metadata": {},
   "source": [
    "### Let's visualize a random image from the training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d778551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in training_generator():\n",
    "    _id = np.random.randint(0, len(x))\n",
    "    plot_example(x[_id], y[_id], IMG_SIZE, ANCHOR_SIZE)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10e9b0",
   "metadata": {},
   "source": [
    "### Exercise 3: Create convolutional block\n",
    "\n",
    "Complete conv_block function below. It's made out of **three (3)** parts Conv layer, batch normalization, and dropout layer. Conv layer has a 3x3 kernel, and padding is equal to \"same\". Dropout should be at 50% (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fd581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(conv_features, inputs, activation = 'relu'):\n",
    "    \"\"\"\n",
    "    Creates a single convolutional block - Conv, batch norm, dropout operations\n",
    "    \n",
    "    Args:\n",
    "        conv_features :int: - Number of features used in the Conv2D layer\n",
    "        inputs - input from the previous layer\n",
    "        activation :string: - Activation function used in the Conv2D laer\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "test_conv_block(conv_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082abc5",
   "metadata": {},
   "source": [
    "### Exercise 4: Create a residual block\n",
    "\n",
    "![Residual block](https://www.researchgate.net/publication/333407474/figure/fig1/AS:763343056941057@1559006575644/The-architecture-of-the-a-residual-block-and-b-ResNet.png)\n",
    "<center>Images taken from: <a href=\"https://www.researchgate.net/figure/The-architecture-of-the-a-residual-block-and-b-ResNet_fig1_333407474\">here</a></center>\n",
    "<br><br>\n",
    "\n",
    "In the cell below, you have the **residual_block** function.\n",
    "The residual block has three conv_blocks inside it, each with the same feature numbers, but mind the inputs! \n",
    "The output of this function is the **Concatenate** function of res_block inputs and last conv outputs.\n",
    "\n",
    "NOTE: Take a look at the picture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(features, inputs):\n",
    "    \"\"\"\n",
    "    Creates a single residual block\n",
    "    \n",
    "    Args:\n",
    "        features :int: - Number of features used in all conv_blocks for this residual_block\n",
    "        inputs - input from the previous layer\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "test_residual_block(residual_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe664e",
   "metadata": {},
   "source": [
    "## SSD Architecture\n",
    "\n",
    "Let's put everything together and build the SSD network!\n",
    "\n",
    "SSD Model is a fully convolutional model; the output layer is also the convolutional layer! Since SSD does not use ROI prediction before it tries to predict everything at the same time. The output layer of SSD is the Conv2D layer with size *[w, h, classes+4]*. \n",
    "\n",
    "Let's break down the last part -  **classes + 4**. The classes are pretty straightforward, but don't forget the background class! So if you have 10 classes in a dataset, this number will be 11! The **4** is for predicting bounding box coordinates.\n",
    "\n",
    "#### Watch this video first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bac523",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/0frKXR-2PBY\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b8613",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case here is the like for YouTube video from cell above: https://www.youtube.com/watch?v=0frKXR-2PBY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe891fbc",
   "metadata": {},
   "source": [
    "There is a lot to learn about SSD. Here are the resources:\n",
    "- https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06\n",
    "- https://developers.arcgis.com/python/guide/how-ssd-works/\n",
    "\n",
    "\n",
    "### Exercise 5: Get better loss than me\n",
    "\n",
    "You can see the architecture I used to prepare this notebook in the cell below, which got me **0.87** loss on the **test** set. Your task here is to optimize this architecture and to get loss lower than **0.85**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape = (IMG_SIZE, IMG_SIZE, 3))\n",
    "# DO NOT CHANE CODE ABOVE THIS LINE\n",
    "\n",
    "block1 = residual_block(16, inp)\n",
    "pool1  = MaxPooling2D(pool_size = (2, 2))(block1)\n",
    "block2 = residual_block(32, pool1)\n",
    "pool2  = MaxPooling2D(pool_size = (2, 2))(block2)\n",
    "block3 = residual_block(64, pool2)\n",
    "pool3  = MaxPooling2D(pool_size = (2, 2))(block3)\n",
    "block4 = residual_block(64, pool3)\n",
    "pool4  = MaxPooling2D(pool_size = (2, 2))(block4)\n",
    "block5 = residual_block(128, pool4)\n",
    "pool5  = MaxPooling2D(pool_size = (2, 2))(block5)\n",
    "\n",
    "# DO NOT CHANGE CODE BELOW THIS LINE!\n",
    "out  = Conv2D(5, (3, 3), padding = 'same', activation = 'sigmoid')(pool5)\n",
    "\n",
    "#create a model with one input and two outputs \n",
    "model = tf.keras.models.Model(inputs = inp, outputs = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb954c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94018812",
   "metadata": {},
   "source": [
    "### Losses\n",
    "\n",
    "When working with SSD there is two parts to it's loss. Confidence loss which checks for classes and detection loss for optimizing over bounding box locations.\n",
    "\n",
    "![](images/losses.png)\n",
    "<center>Images taken from: <a href=\"https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab\">here</a></center>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7083a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, y):\n",
    "    \"\"\"\n",
    "    Custom loss function used to optimize SSD algorithm.\n",
    "    This loss has two parts: Confidence loss and Location loss\n",
    "    \n",
    "    Read more about this loss here: https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab\n",
    "    \n",
    "    Args:\n",
    "        preds - Model predictions\n",
    "        y - real targets from a dataset\n",
    "    \"\"\"\n",
    "    # Choose only bounding boxes where class is NOT zero (i.e not background)\n",
    "    mask = y[..., 0]\n",
    "    boxA    = tf.boolean_mask(y, mask)\n",
    "    boxB    = tf.boolean_mask(pred, mask)\n",
    "\n",
    "    # Confidence loss\n",
    "    prediction_error = tf.keras.losses.binary_crossentropy(y[..., 0], pred[..., 0])\n",
    "    \n",
    "    # Location loss\n",
    "    detection_error = tf.compat.v1.losses.absolute_difference(boxA[..., 1:], boxB[..., 1:])  \n",
    "    \n",
    "    # The number 10 is taken from the SSD paper\n",
    "    return tf.reduce_mean(prediction_error) + 10 * detection_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f40616",
   "metadata": {},
   "source": [
    "## Calculating gradients\n",
    "\n",
    "Since we are using a special loss function for our network, the optimization part is weird. We can't simply call *model.fit()*, but we need to create a custom training loop and calculate gradients semi-manually.\n",
    "\n",
    "Link to read more about GradientTape method: https://www.tensorflow.org/guide/autodiff#gradient_tapes\n",
    "\n",
    "\n",
    "### Exercise 6: Complete function for calculating gradients\n",
    "\n",
    "By following the tutorials from the [link](https://www.tensorflow.org/guide/autodiff#gradient_tapes) complete the **grad** function.\n",
    "\n",
    "Steps:\n",
    "- Define gradient taping context\n",
    "- Make predictions with a model\n",
    "- Calculate loss\n",
    "- Return gradients of all trainable_variables over loss AND loss itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb93fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, x, y):\n",
    "    \"\"\"\n",
    "    Custom function for calculating gradients with respect to the loss function\n",
    "    \n",
    "    Args:\n",
    "        model - Keras model that's being trained\n",
    "        x - dataset features (inputs)\n",
    "        y - real targets (also from a dataset)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Calculate loss based on the input and predictions\n",
    "    \n",
    "        \n",
    "    # Return the gradient of the loss function with respect to the model variables \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35ccc0",
   "metadata": {},
   "source": [
    "### Exercise 7: Define optimizer\n",
    "\n",
    "Define a custom optimizer for the model in the cell below. \n",
    "\n",
    "NOTE: I've used **Adam** optimizer and it showed okay results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8813778",
   "metadata": {},
   "source": [
    "### Exercise 8: Complete the training loop\n",
    "\n",
    "You'll find a training loop in the cell below that runs through training/testing data and saves the model if the model performed better over testing portion of the dataset.\n",
    "\n",
    "Your task is to complete it by applying calculated gradients from the grad method. There are **three (3)** lines of code you should define:\n",
    "\n",
    "- Call grad method and get gradients and loss value\n",
    "- Apply gradients on the optimizer object (Defined in the *Exercise 7*)\n",
    "- Append loss to the training loss list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6368c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "best_loss = 1.0\n",
    "for i in range(epochs):\n",
    "        \n",
    "    epoch_train_loss = []\n",
    "    epoch_test_loss = []\n",
    "    for x, y in training_generator():\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "    # CODE ABOVE THIS LINE IS MENDETORY\n",
    "    # CODE BELOW THIS LINE IS OPTIONAL! BUT USE IT IF YOU WANT TO SAVE MODEL\n",
    "    for x, y in test_generator():\n",
    "        pred = model(x)\n",
    "        l = loss(pred, y)\n",
    "        epoch_test_loss.append(l.numpy())\n",
    "        \n",
    "    # aggregate losses\n",
    "    current_train_loss = np.mean(epoch_train_loss)\n",
    "    current_test_loss = np.mean(epoch_test_loss)\n",
    "    \n",
    "    # create history\n",
    "    train_loss.append(current_train_loss)\n",
    "    test_loss.append(current_test_loss)\n",
    "    \n",
    "    # Get some results printed\n",
    "    print(\"Epoch {0:d}/{1:d}: Training loss: {2:0.4f}, Testing loss: {3:0.4f}\".format(i+1, epochs, current_train_loss, \n",
    "              current_test_loss))\n",
    "    \n",
    "    # Check if there is a better model and save it\n",
    "    if current_test_loss  < best_loss:\n",
    "        best_loss = current_test_loss\n",
    "        print('Model saved')\n",
    "        model.save('model_save.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0918e", 
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot run\n",
    "nepochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(nepochs, train_loss, \"ro\", label=\"train loss\")\n",
    "plt.plot(nepochs, test_loss, \"b*\", label=\"test loss\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
 ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e04cb",
   "metadata": {},
   "source": [
    "## Loading model and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67029e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('model_save.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = np.random.choice(X_test)\n",
    "plot_pred(best_model, img_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

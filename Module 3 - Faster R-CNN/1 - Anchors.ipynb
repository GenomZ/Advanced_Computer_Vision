{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e626f0",
   "metadata": {},
   "source": [
    "# Faster R-CNN\n",
    "\n",
    "\n",
    "![Faster R-CNN](https://lilianweng.github.io/lil-log/assets/images/faster-RCNN.png)\n",
    "<center>Image taken from <a href=\"https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html0\">here</a></center>\n",
    "<br><br>\n",
    "\n",
    "So far, we talked about R-CNN and the logical speedup to do at this point is the way we select ROIs for our network. The ROI selection process is completed even before we put those proposals through the CNN part of the architecture. To speed things up, we would like to make it a part of the network itself! That's exactly the main difference that Faster R-CNN introduced in its architecture.\n",
    "\n",
    "The network looks pretty similar to what we have had so far. We have a pre-trained network (e.g., VGG16), which we use for a CNN part, and the RPN (Region proposal network) to generate ROIs for the full model training. The step by step model flow is explained pretty well here: \n",
    "\n",
    "https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html\n",
    "\n",
    "Overall, we have 3 new things introduced for the Faster R-CNN model:\n",
    "\n",
    "- ### Anchor boxes\n",
    "\n",
    "- ### Region Proposal Network (RPN)\n",
    "\n",
    "- ### ROI pooling \n",
    "\n",
    "In this section of the course, we will cover all core techniques that make a Faster R-CNN model. Since there is a slight change in architecture which we already trained in the previous lessons, we will implement only the differences and provide you great resources to implement the entire model on your own time, for your data! However, the full implementation of the model (including pre-processing, post-processing) is very long and might take several days/weeks to complete. \n",
    "\n",
    "https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/\n",
    "https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/\n",
    "https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a\n",
    "\n",
    "\n",
    "### Steps:\n",
    "1. Import dependencies\n",
    "2. Define the make_anchors\n",
    "\n",
    "### Topics covered and learning objectives\n",
    "- Anchor boxes for object detection\n",
    "\n",
    "### Time estimates:\n",
    "- Reading/Watching materials: 20min\n",
    "- Exercises: 10min\n",
    "<br><br>\n",
    "- **Total**: ~30min\n",
    "\n",
    "\n",
    "\n",
    "# Anchor boxes\n",
    "\n",
    "![](https://ww2.mathworks.cn/help/vision/ug/ssd_detection.png)\n",
    "<center>Image taken from <a href=\"https://ww2.mathworks.cn/help/vision/ug/getting-started-with-ssd.html\">here</a></center>\n",
    "<br><br>\n",
    "In the previous tries to make the best object detection algorithm, we used randomly generated ROIs (including positions and sizes). This provided decent results, but since everything was random, it was not our control, and we very much relayed on luck. To handle this, people introduced **anchor boxes**.\n",
    "\n",
    "    Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect, and are typically chosen based on object sizes in your training datasets. During detection, the predefined anchor boxes are tiled across the image. The network predicts the probability and other attributes, such as background, intersection over union (IoU), and offsets for every tiled anchor box. The predictions are used to refine each anchor box. You can define several anchor boxes, each for different object size. Anchor boxes are fixed initial-boundary box guesses.\n",
    "Taken from [here](https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html)\n",
    "\n",
    "\n",
    "When we tile them, they might look something like this:\n",
    "\n",
    "![](https://dongjk.github.io/assets/article_images/2018-05-21-Faster_R-CNN_step_by_step/all_inside_anchors.jpg)\n",
    "<center>Image taken from <a href=\"https://dongjk.github.io/code/object+detection/keras/2018/05/21/Faster_R-CNN_step_by_step,_Part_I.html\">here</a></center>\n",
    "<br><br>\n",
    "\n",
    "### Anchor box resources:\n",
    "\n",
    "- Quality Object detection: https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9\n",
    "- Anchors explained: https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95575bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/RTlwl2bv0Tg\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c381ad",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case here is the like for YouTube video from the cell above: https://www.youtube.com/watch?v=RTlwl2bv0Tg\n",
    "\n",
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98100808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tests import test_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a2899",
   "metadata": {},
   "source": [
    "### Exercise 1: Complete the function that generates anchors\n",
    "\n",
    "This function was created as a reference to the one found [here](https://d2l.ai/chapter_computer-vision/anchor.html). \n",
    "It creates a grid on top of an image and generates anchor boxes for each grid of an image. \n",
    "\n",
    "NOTE: If you define the grid_cell_size to be 1, this function will perform pixel-wise generation of anchor boxes.\n",
    "\n",
    "The first part of the task is to fill a couple of lines of code for the **make_anchors** function. Here is how:\n",
    "\n",
    "- Step 1: Calculate the number of grid cells for X-axis and Y-axis. **grid_cell_size** and **height, width** are already defined\n",
    "\n",
    "- Step 2: Using np.arange function, find central points of each grid cell in the X-axis and Y-axis\n",
    "    - HINT: you can use window_size/2 as a starting point\n",
    "    - Use documentation of np.arange to find what arguments to provide\n",
    "   \n",
    "- Step 3: Using np.stack function to stack together this list: *[center_x, center_y, center_x, center_y]* along axis one,  then repeat that block for the number of boxes per grid cells, along the axis zero.\n",
    "\n",
    "\n",
    "After completing the first 3 steps and passing the test, go to the source code found [here](https://d2l.ai/chapter_computer-vision/anchor.html) walk through the implementation. This will take 10-15min on average. The goal is to understand different approaches to implementing anchor boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_anchors(img_size, \n",
    "                 sizes, \n",
    "                 ratios, \n",
    "                 grid_cell_size=100):\n",
    "    \"\"\"\n",
    "    Calculate anchor box proposals per part of the image.\n",
    "    \n",
    "    Args:\n",
    "        :param img_size (tuple): WxH of images in a dataset E.g. (512, 512)\n",
    "        :param sizes (np.array): array of sizes of bboxes\n",
    "        :param ratios (np.array): array of ratios of bboxes\n",
    "        :param grid_cell_size (int): size of a grid cell\n",
    "    \"\"\"\n",
    "    height = img_size[0]\n",
    "    width = img_size[1]\n",
    "    \n",
    "    grid_cell_size = grid_cell_size\n",
    "    num_sizes = len(sizes)\n",
    "    num_ratios = len(ratios)\n",
    "    boxes_per_grid_cell = (num_sizes + num_ratios - 1) # Note: NOT num_sizes * num_ratios, see eq 14.4.1 in the link\n",
    "    \n",
    "    # Step 1: YOUR CODE HERE\n",
    "    grid_x=None\n",
    "    grid_y=None\n",
    "    \n",
    "    # Step 2: YOUR CODE HERE\n",
    "    center_h = None\n",
    "    center_w = None\n",
    "    \n",
    "    # Put center coordinates of each grid cell into a matrix \n",
    "    center_x, center_y = np.meshgrid(center_w, center_h)\n",
    "    center_x, center_y = center_x.reshape(-1), center_y.reshape(-1)\n",
    "    \n",
    "    \n",
    "    # Taken from: https://d2l.ai/chapter_computer-vision/anchor.html\n",
    "    # This part of the code calculates ratios and sizes of anchor boxes\n",
    "    w = np.concatenate((sizes * np.sqrt(ratios[0]), \n",
    "                        sizes[0] * np.sqrt(ratios[1:]))) * grid_cell_size  \n",
    "    h = np.concatenate((sizes / np.sqrt(ratios[0]),\n",
    "                        sizes[0] / np.sqrt(ratios[1:]))) * grid_cell_size\n",
    "    \n",
    "    # Get all calculates \n",
    "    anchor_manipulations = np.tile(np.stack((-w, -h, w, h)).T, ((grid_x * grid_y, 1))) / 2\n",
    "    \n",
    "    # Step 3: YOUR CODE HERE:\n",
    "    # Stack center points together and make a final grind\n",
    "    final_grid = None\n",
    "    \n",
    "    # Add new coordinates and generate new position\n",
    "    output = final_grid + anchor_manipulations\n",
    "    \n",
    "    return np.expand_dims(output, axis=0).reshape(grid_x, grid_y, boxes_per_grid_cell, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da918a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO TEST YOUR IMPLEMENTATION\n",
    "test_anchors(make_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca80694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
